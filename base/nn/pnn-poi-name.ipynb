{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Lambda, Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Dropout, Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras import backend as K\n",
    "from sklearn import metrics\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "context_train = []\n",
    "ad_train = []\n",
    "y_train = []\n",
    "with open('../../data/click-train') as f:\n",
    "    for line in f:\n",
    "        label, context, ad = line.strip().split(\"\\t\")\n",
    "        y_train.append(int(label))\n",
    "        context_train.append(map(lambda x: int(x), context.split(\" \")))\n",
    "        ad_train.append(map(lambda x: int(x), ad.split(\" \")))\n",
    "        \n",
    "        \n",
    "context_test = []\n",
    "ad_test = []\n",
    "y_test = []\n",
    "with open('../../data/click-test') as f:\n",
    "    for line in f:\n",
    "        label, context, ad = line.strip().split(\"\\t\")\n",
    "        y_test.append(int(label))\n",
    "        context_test.append(map(lambda x: int(x), context.split(\" \")))\n",
    "        ad_test.append(map(lambda x: int(x), ad.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_features = 2 ** 17\n",
    "maxlen = 15\n",
    "\n",
    "\n",
    "context_train = sequence.pad_sequences(context_train, maxlen=maxlen)\n",
    "ad_train = sequence.pad_sequences(ad_train, maxlen=maxlen)\n",
    "\n",
    "context_test = sequence.pad_sequences(context_test, maxlen=maxlen)\n",
    "ad_test = sequence.pad_sequences(ad_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def means(x):\n",
    "    return K.mean(x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1992379 samples, validate on 250711 samples\n",
      "Epoch 1/1\n",
      "4008s - loss: 0.6841 - fmeasure: 0.5514 - val_loss: 0.6791 - val_fmeasure: 0.5489\n",
      "train auc -> 0.623699103788\n",
      "test auc -> 0.594189722196\n",
      "Train on 1992379 samples, validate on 250711 samples\n",
      "Epoch 1/1\n",
      "5169s - loss: 0.6741 - fmeasure: 0.5750 - val_loss: 0.6769 - val_fmeasure: 0.5659\n",
      "train auc -> 0.644252701528\n",
      "test auc -> 0.600620696083\n",
      "Train on 1992379 samples, validate on 250711 samples\n",
      "Epoch 1/1\n",
      "5703s - loss: 0.6684 - fmeasure: 0.5861 - val_loss: 0.6763 - val_fmeasure: 0.5653\n",
      "train auc -> 0.65980500993\n",
      "test auc -> 0.603117810094\n",
      "Train on 1992379 samples, validate on 250711 samples\n",
      "Epoch 1/1\n",
      "6015s - loss: 0.6637 - fmeasure: 0.5939 - val_loss: 0.6759 - val_fmeasure: 0.5636\n",
      "train auc -> 0.676247089719\n",
      "test auc -> 0.604000503029\n",
      "Train on 1992379 samples, validate on 250711 samples\n",
      "Epoch 1/1\n",
      "6143s - loss: 0.6591 - fmeasure: 0.6024 - val_loss: 0.6775 - val_fmeasure: 0.5796\n",
      "train auc -> 0.688588137821\n",
      "test auc -> 0.604093268294\n",
      "Train on 1992379 samples, validate on 250711 samples\n",
      "Epoch 1/1\n",
      "6236s - loss: 0.6551 - fmeasure: 0.6086 - val_loss: 0.6777 - val_fmeasure: 0.5649\n",
      "train auc -> 0.701187377191\n",
      "test auc -> 0.60339159873\n",
      "Train on 1992379 samples, validate on 250711 samples\n",
      "Epoch 1/1\n",
      "6289s - loss: 0.6511 - fmeasure: 0.6149 - val_loss: 0.6793 - val_fmeasure: 0.5627\n",
      "train auc -> 0.716193705365\n",
      "test auc -> 0.601705481561\n",
      "Train on 1992379 samples, validate on 250711 samples\n",
      "Epoch 1/1\n",
      "6345s - loss: 0.6473 - fmeasure: 0.6203 - val_loss: 0.6815 - val_fmeasure: 0.5705\n",
      "train auc -> 0.72276334149\n",
      "test auc -> 0.602151218572\n",
      "Train on 1992379 samples, validate on 250711 samples\n",
      "Epoch 1/1\n",
      "6409s - loss: 0.6434 - fmeasure: 0.6260 - val_loss: 0.6827 - val_fmeasure: 0.5582\n",
      "train auc -> 0.736882653162\n",
      "test auc -> 0.601530447563\n",
      "Train on 1992379 samples, validate on 250711 samples\n",
      "Epoch 1/1\n",
      "6439s - loss: 0.6398 - fmeasure: 0.6308 - val_loss: 0.6839 - val_fmeasure: 0.5817\n",
      "train auc -> 0.746433365356\n",
      "test auc -> 0.599386508142\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Merge\n",
    "\n",
    "batch_size = 32\n",
    "embedding_dims = 50\n",
    "\n",
    "\n",
    "left_branch = Sequential()\n",
    "left_branch.add(Embedding(input_dim=max_features, output_dim=embedding_dims, init='glorot_uniform'))\n",
    "left_branch.add(Dropout(0.5))\n",
    "left_branch.add(Lambda(means, output_shape=(embedding_dims,)))\n",
    "\n",
    "right_branch = Sequential()\n",
    "right_branch.add(Embedding(input_dim=max_features, output_dim=embedding_dims, init='glorot_uniform'))\n",
    "left_branch.add(Dropout(0.5))\n",
    "right_branch.add(Lambda(means, output_shape=(embedding_dims,)))\n",
    "\n",
    "\n",
    "merged = Merge([left_branch, right_branch], mode=lambda l: l[0] * l[1], output_shape=[embedding_dims])\n",
    "\n",
    "\n",
    "final_model = Sequential()\n",
    "final_model.add(merged)\n",
    "\n",
    "final_model.add(Dense(20))\n",
    "final_model.add(BatchNormalization())\n",
    "final_model.add(LeakyReLU(alpha=.01))\n",
    "final_model.add(Dense(1))\n",
    "final_model.add(BatchNormalization())\n",
    "final_model.add(Activation('sigmoid'))\n",
    "\n",
    "final_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['fmeasure'])\n",
    "\n",
    "model_path = ''\n",
    "for i in range(10):\n",
    "    if model_path != '':\n",
    "        final_model.load_weights(model_path)\n",
    "    model_path = '../../model/weights.epoch' + str(i) + \"_\" + str(int(time.time())) + '.hdf5'\n",
    "    weight_save_callback = ModelCheckpoint(model_path, monitor='val_loss', verbose=0, save_best_only=False, mode='auto')\n",
    "    final_model.fit([context_train, ad_train], y_train, verbose=2, batch_size=batch_size,validation_data=([context_test, ad_test], y_test), nb_epoch=1, callbacks=[weight_save_callback])\n",
    "    \n",
    "    y_pred = final_model.predict([context_train, ad_train])[:, 0]\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_train, y_pred)\n",
    "    print \"train auc -> \" + str(metrics.auc(fpr, tpr))\n",
    "    \n",
    "    y_pred = final_model.predict([context_test, ad_test])[:, 0]\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "    print \"test auc -> \" + str(metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 50)\n",
      "[(None, 50), (None, 50)]\n",
      "(None, 50)\n"
     ]
    }
   ],
   "source": [
    "print merged.output_shape\n",
    "print merged.input_shape\n",
    "print left_branch.output_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-fb004386d02f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvisualize_util\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'model.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/opt/meituan/apps/venv/lib/python2.7/site-packages/keras/utils/visualize_util.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mimport\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mpydot\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_graphviz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     raise RuntimeError('Failed to import pydot. You must install pydot'\n\u001b[0m\u001b[0;32m     14\u001b[0m                        ' and graphviz for `pydotprint` to work.')\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work."
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from keras.layers import Merge\n",
    "\n",
    "batch_size = 32\n",
    "embedding_dims = 100\n",
    "\n",
    "\n",
    "left_branch = Sequential()\n",
    "left_branch.add(Embedding(input_dim=max_features, output_dim=embedding_dims, init='glorot_uniform'))\n",
    "# left_branch.add(BatchNormalization())\n",
    "left_branch.add(Dropout(0.5))\n",
    "left_branch.add(Lambda(means, output_shape=(embedding_dims,)))\n",
    "# left_branch.add(Dense(32, input_dim=embeding_dim))\n",
    "\n",
    "right_branch = Sequential()\n",
    "right_branch.add(Embedding(input_dim=max_features, output_dim=embedding_dims, init='glorot_uniform'))\n",
    "# left_branch.add(BatchNormalization())\n",
    "left_branch.add(Dropout(0.5))\n",
    "right_branch.add(Lambda(means, output_shape=(embedding_dims,)))\n",
    "# right_branch.add(Dense(32, input_dim=embeding_dim))\n",
    "\n",
    "merged = Merge([left_branch, right_branch], mode='concat')\n",
    "\n",
    "final_model = Sequential()\n",
    "final_model.add(merged)\n",
    "\n",
    "final_model.add(Dense(20))\n",
    "final_model.add(BatchNormalization())\n",
    "final_model.add(LeakyReLU(alpha=.01))\n",
    "final_model.add(Dense(1))\n",
    "final_model.add(BatchNormalization())\n",
    "final_model.add(Activation('sigmoid'))\n",
    "\n",
    "final_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['fmeasure'])\n",
    "\n",
    "\n",
    "from keras.utils.visualize_util import plot\n",
    "plot(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load model to predict\n",
    "model_path = '../../model/weights.epoch3_1481833706.hdf5'\n",
    "final_model.load_weights(model_path)\n",
    "\n",
    "y_pred = final_model.predict([context_train, ad_train])[:, 0]\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_train, y_pred)\n",
    "print \"train auc -> \" + str(metrics.auc(fpr, tpr))\n",
    "\n",
    "y_pred = final_model.predict([context_test, ad_test])[:, 0]\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "print \"test auc -> \" + str(metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! ls ../../modelweights.epoch3_1481833706.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}