{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Lambda, Embedding\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import Dropout, Activation\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras import backend as K\n",
    "from sklearn import metrics\n",
    "from keras.regularizers import WeightRegularizer\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "dummy_empty = 2 ** 17\n",
    "\n",
    "context_name_train = []\n",
    "context_type_train = []\n",
    "context_category_train = []\n",
    "context_barea_train = []\n",
    "context_address_train = []\n",
    "\n",
    "ad_name_train = []\n",
    "ad_type_train = []\n",
    "ad_category_train = []\n",
    "ad_barea_train = []\n",
    "ad_address_train = []\n",
    "\n",
    "y_train = []\n",
    "\n",
    "\n",
    "with open('../../data/click-all-train') as f:\n",
    "    for line in f:\n",
    "        label, context_name, context_type, context_category, context_barea, context_address, ad_name, ad_type, ad_category, ad_barea, ad_address = line.split(\"\\t\")\n",
    "        y_train.append(int(label))\n",
    "        context_name_train.append(map(lambda x: int(x) if x else dummy_empty, context_name.strip().split(\" \")))\n",
    "        context_type_train.append(map(lambda x: int(x) if x else dummy_empty, context_type.strip().split(\" \")))\n",
    "        context_category_train.append(map(lambda x: int(x) if x else dummy_empty, context_category.strip().split(\" \")))\n",
    "        context_barea_train.append(map(lambda x:  int(x) if x else dummy_empty, context_barea.strip().split(\" \")))\n",
    "        context_address_train.append(map(lambda x:  int(x) if x else dummy_empty, context_address.strip().split(\" \")))\n",
    "        ad_name_train.append(map(lambda x: int(x) if x else dummy_empty, ad_name.strip().split(\" \")))\n",
    "        ad_type_train.append(map(lambda x: int(x) if x else dummy_empty, ad_type.strip().split(\" \")))\n",
    "        ad_category_train.append(map(lambda x: int(x) if x else dummy_empty, ad_category.strip().split(\" \")))\n",
    "        ad_barea_train.append(map(lambda x: int(x) if x else dummy_empty, ad_barea.strip().split(\" \")))\n",
    "        ad_address_train.append(map(lambda x: int(x) if x else dummy_empty, ad_address.strip().split(\" \")))\n",
    "\n",
    "\n",
    "context_name_test = []\n",
    "context_type_test = []\n",
    "context_category_test = []\n",
    "context_barea_test = []\n",
    "context_address_test = []\n",
    "\n",
    "\n",
    "ad_name_test = []\n",
    "ad_type_test = []\n",
    "ad_category_test = []\n",
    "ad_barea_test = []\n",
    "ad_address_test = []\n",
    "\n",
    "\n",
    "y_test = []\n",
    "with open('../../data/click-all-test') as f:\n",
    "    for line in f:\n",
    "        label, context_name, context_type, context_category, context_barea, context_address, ad_name, ad_type, ad_category, ad_barea, ad_address = line.split(\"\\t\")\n",
    "        y_test.append(int(label))\n",
    "        context_name_test.append(map(lambda x: int(x) if x else dummy_empty, context_name.strip().split(\" \")))\n",
    "        context_type_test.append(map(lambda x:  int(x) if x else dummy_empty, context_type.strip().split(\" \")))\n",
    "        context_category_test.append(map(lambda x:  int(x) if x else dummy_empty, context_category.strip().split(\" \")))\n",
    "        context_barea_test.append(map(lambda x: int(x) if x else dummy_empty, context_barea.strip().split(\" \")))\n",
    "        context_address_test.append(map(lambda x:  int(x) if x else dummy_empty, context_address.strip().split(\" \")))\n",
    "        ad_name_test.append(map(lambda x:  int(x) if x else dummy_empty, ad_name.strip().split(\" \")))\n",
    "        ad_type_test.append(map(lambda x:  int(x) if x else dummy_empty, ad_type.strip().split(\" \")))\n",
    "        ad_category_test.append(map(lambda x:  int(x) if x else dummy_empty, ad_category.strip().split(\" \")))\n",
    "        ad_barea_test.append(map(lambda x:  int(x) if x else dummy_empty, ad_barea.strip().split(\" \")))\n",
    "        ad_address_test.append(map(lambda x:  int(x) if x else dummy_empty, ad_address.strip().split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "maxlen = 15\n",
    "\n",
    "\n",
    "context_name_train = sequence.pad_sequences(context_name_train, maxlen=maxlen)\n",
    "context_type_train = sequence.pad_sequences(context_type_train, maxlen=maxlen)\n",
    "context_category_train = sequence.pad_sequences(context_category_train, maxlen=maxlen)\n",
    "context_barea_train = sequence.pad_sequences(context_barea_train, maxlen=maxlen)\n",
    "context_address_train = sequence.pad_sequences(context_address_train, maxlen=maxlen)\n",
    "\n",
    "ad_name_train = sequence.pad_sequences(ad_name_train, maxlen=maxlen)\n",
    "ad_type_train = sequence.pad_sequences(ad_type_train, maxlen=maxlen)\n",
    "ad_category_train = sequence.pad_sequences(ad_category_train, maxlen=maxlen)\n",
    "ad_barea_train = sequence.pad_sequences(ad_barea_train, maxlen=maxlen)\n",
    "ad_address_train = sequence.pad_sequences(ad_address_train, maxlen=maxlen)\n",
    "\n",
    "\n",
    "context_name_test = sequence.pad_sequences(context_name_test, maxlen=maxlen)\n",
    "context_type_test = sequence.pad_sequences(context_type_test, maxlen=maxlen)\n",
    "context_category_test = sequence.pad_sequences(context_category_test, maxlen=maxlen)\n",
    "context_barea_test = sequence.pad_sequences(context_barea_test, maxlen=maxlen)\n",
    "context_address_test = sequence.pad_sequences(context_address_test, maxlen=maxlen)\n",
    "\n",
    "ad_name_test = sequence.pad_sequences(ad_name_test, maxlen=maxlen)\n",
    "ad_type_test = sequence.pad_sequences(ad_type_test, maxlen=maxlen)\n",
    "ad_category_test = sequence.pad_sequences(ad_category_test, maxlen=maxlen)\n",
    "ad_barea_test = sequence.pad_sequences(ad_barea_test, maxlen=maxlen)\n",
    "ad_address_test = sequence.pad_sequences(ad_address_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def means(x):\n",
    "    return K.mean(x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1993177 samples, validate on 250766 samples\n",
      "Epoch 1/1\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Merge\n",
    "\n",
    "max_features = 2 ** 17 + 1\n",
    "batch_size = 32\n",
    "embedding_dims = 20\n",
    "\n",
    "\n",
    "context_name = Sequential()\n",
    "context_name.add(Embedding(input_dim=max_features, output_dim=embedding_dims, init='glorot_uniform', dropout=0.5))\n",
    "# context_name.add(Dropout(0.5))\n",
    "context_name.add(Lambda(means, output_shape=(embedding_dims,)))\n",
    "\n",
    "context_type = Sequential()\n",
    "context_type.add(Embedding(input_dim=max_features, output_dim=embedding_dims, init='glorot_uniform', dropout=0.5))\n",
    "# context_type.add(Dropout(0.5))\n",
    "context_type.add(Lambda(means, output_shape=(embedding_dims,)))\n",
    "\n",
    "context_category = Sequential()\n",
    "context_category.add(Embedding(input_dim=max_features, output_dim=embedding_dims, init='glorot_uniform', dropout=0.5))\n",
    "# context_category.add(Dropout(0.5))\n",
    "context_category.add(Lambda(means, output_shape=(embedding_dims,)))\n",
    "\n",
    "context_barea = Sequential()\n",
    "context_barea.add(Embedding(input_dim=max_features, output_dim=embedding_dims, init='glorot_uniform', dropout=0.5))\n",
    "# context_barea.add(Dropout(0.5))\n",
    "context_barea.add(Lambda(means, output_shape=(embedding_dims,)))\n",
    "\n",
    "context_address = Sequential()\n",
    "context_address.add(Embedding(input_dim=max_features, output_dim=embedding_dims, init='glorot_uniform', dropout=0.5))\n",
    "# context_address.add(Dropout(0.5))\n",
    "context_address.add(Lambda(means, output_shape=(embedding_dims,)))\n",
    "\n",
    "ad_name = Sequential()\n",
    "ad_name.add(Embedding(input_dim=max_features, output_dim=embedding_dims, init='glorot_uniform', dropout=0.5))\n",
    "ad_name.add(Dropout(0.5))\n",
    "ad_name.add(Lambda(means, output_shape=(embedding_dims,)))\n",
    "\n",
    "ad_type = Sequential()\n",
    "ad_type.add(Embedding(input_dim=max_features, output_dim=embedding_dims, init='glorot_uniform', dropout=0.5))\n",
    "ad_type.add(Dropout(0.5))\n",
    "ad_type.add(Lambda(means, output_shape=(embedding_dims,)))\n",
    "\n",
    "ad_category = Sequential()\n",
    "ad_category.add(Embedding(input_dim=max_features, output_dim=embedding_dims, init='glorot_uniform', dropout=0.5))\n",
    "ad_category.add(Dropout(0.5))\n",
    "ad_category.add(Lambda(means, output_shape=(embedding_dims,)))\n",
    "\n",
    "ad_barea = Sequential()\n",
    "ad_barea.add(Embedding(input_dim=max_features, output_dim=embedding_dims, init='glorot_uniform', dropout=0.5))\n",
    "ad_barea.add(Dropout(0.5))\n",
    "ad_barea.add(Lambda(means, output_shape=(embedding_dims,)))\n",
    "\n",
    "ad_address = Sequential()\n",
    "ad_address.add(Embedding(input_dim=max_features, output_dim=embedding_dims, init='glorot_uniform', dropout=0.5))\n",
    "ad_address.add(Dropout(0.5))\n",
    "ad_address.add(Lambda(means, output_shape=(embedding_dims,)))\n",
    "\n",
    "\n",
    "\n",
    "merged = Merge([context_name, context_type, context_category, context_barea, context_address, \\\n",
    "               ad_name, ad_type, ad_category, ad_barea, ad_address], mode='concat')\n",
    "\n",
    "final_model = Sequential()\n",
    "final_model.add(merged)\n",
    "\n",
    "final_model.add(Dense(20))\n",
    "final_model.add(BatchNormalization())\n",
    "final_model.add(LeakyReLU(alpha=.01))\n",
    "final_model.add(Dropout(0.5))\n",
    "final_model.add(Dense(1))\n",
    "final_model.add(BatchNormalization())\n",
    "final_model.add(Activation('sigmoid'))\n",
    "\n",
    "final_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['fmeasure'])\n",
    "\n",
    "model_path = ''\n",
    "for i in range(10):\n",
    "    if model_path != '':\n",
    "        final_model.load_weights(model_path)\n",
    "    model_path = '../../model/weights.epoch' + str(i) + \"_\" + str(int(time.time())) + '.hdf5'\n",
    "    weight_save_callback = ModelCheckpoint(model_path, monitor='val_loss', verbose=0, save_best_only=False, mode='auto')\n",
    "    final_model.fit([context_name_train, context_type_train, context_category_train, context_barea_train, context_address_train, \\\n",
    "                     ad_name_train, ad_type_train, ad_category_train, ad_barea_train, ad_address_train], y_train,  batch_size=batch_size, verbose=2, validation_data=([context_name_test, context_type_test, context_category_test, context_barea_test, context_address_test, \\\n",
    "                     ad_name_test, ad_type_test, ad_category_test, ad_barea_test, ad_address_test], y_test), nb_epoch=1, callbacks=[weight_save_callback])\n",
    "    \n",
    "    y_pred = final_model.predict([context_name_train, context_type_train, context_category_train, context_barea_train, context_address_train, \\\n",
    "                     ad_name_train, ad_type_train, ad_category_train, ad_barea_train, ad_address_train])[:, 0]\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_train, y_pred)\n",
    "    print \"train auc -> \" + str(metrics.auc(fpr, tpr))\n",
    "    \n",
    "    y_pred = final_model.predict([context_name_test, context_type_test, context_category_test, context_barea_test, context_address_test, \\\n",
    "                     ad_name_test, ad_type_test, ad_category_test, ad_barea_test, ad_address_test])[:, 0]\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "    print \"test auc -> \" + str(metrics.auc(fpr, tpr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_model.fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# final_model.load_weights('../../model/weights.01-0.69.hdf5')\n",
    "# final_model.fit([context_train, ad_train], y_train, verbose=2, batch_size=batch_size,validation_data=([context_test, ad_test], y_test), nb_epoch=2, callbacks=[weight_save_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from keras.layers import Merge\n",
    "\n",
    "# batch_size = 32\n",
    "# embedding_dims = 100\n",
    "\n",
    "\n",
    "# left_branch = Sequential()\n",
    "# left_branch.add(Embedding(input_dim=max_features, output_dim=embedding_dims, init='glorot_uniform'))\n",
    "# # left_branch.add(BatchNormalization())\n",
    "# left_branch.add(Dropout(0.5))\n",
    "# left_branch.add(Lambda(means, output_shape=(embedding_dims,)))\n",
    "# # left_branch.add(Dense(32, input_dim=embeding_dim))\n",
    "\n",
    "# right_branch = Sequential()\n",
    "# right_branch.add(Embedding(input_dim=max_features, output_dim=embedding_dims, init='glorot_uniform'))\n",
    "# # left_branch.add(BatchNormalization())\n",
    "# left_branch.add(Dropout(0.5))\n",
    "# right_branch.add(Lambda(means, output_shape=(embedding_dims,)))\n",
    "# # right_branch.add(Dense(32, input_dim=embeding_dim))\n",
    "\n",
    "# merged = Merge([left_branch, right_branch], mode='concat')\n",
    "\n",
    "# final_model = Sequential()\n",
    "# final_model.add(merged)\n",
    "\n",
    "# final_model.add(Dense(20))\n",
    "# final_model.add(BatchNormalization())\n",
    "# final_model.add(LeakyReLU(alpha=.01))\n",
    "# final_model.add(Dense(1))\n",
    "# final_model.add(BatchNormalization())\n",
    "# final_model.add(Activation('sigmoid'))\n",
    "\n",
    "# final_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['fmeasure'])\n",
    "\n",
    "\n",
    "# from keras.utils.visualize_util import plot\n",
    "# plot(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # load model to predict\n",
    "# model_path = '../../model/weights.epoch3_1481833706.hdf5'\n",
    "# final_model.load_weights(model_path)\n",
    "\n",
    "# y_pred = final_model.predict([context_train, ad_train])[:, 0]\n",
    "# fpr, tpr, thresholds = metrics.roc_curve(y_train, y_pred)\n",
    "# print \"train auc -> \" + str(metrics.auc(fpr, tpr))\n",
    "\n",
    "# y_pred = final_model.predict([context_test, ad_test])[:, 0]\n",
    "# fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "# print \"test auc -> \" + str(metrics.auc(fpr, tpr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ! ls ../../modelweights.epoch3_1481833706.hdf5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val = [1,2,3]\n",
    "var = K.variable(value=val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "K.int_shape(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = var * var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
